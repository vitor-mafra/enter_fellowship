# Enter AI Fellowship - Take Home Project

**ExtraÃ§Ã£o de informaÃ§Ã£o de documentos** se tornou trivial com LLMs. O verdadeiro desafio agora Ã© fazer isso **de forma barata, rÃ¡pida e eficiente**. Este projeto Ã© uma tentativa de seguir nessa direÃ§Ã£o: transformar a extraÃ§Ã£o de documentos em um processo adaptativo, que comeÃ§a de mÃ£os dadas com os modelos de linguagem, mas depois aprende a seguir sozinho de maneira muito mais Ã¡gil e econÃ´mica.

Roubando o poder de extrapolaÃ§Ã£o dos modelos de linguagem, o sistema aprende com cada interaÃ§Ã£o. Com uma combinaÃ§Ã£o de hashing perceptual, heurÃ­sticas espaciais e aprendizado incremental, ele reconhece automaticamente quando um documento pertence a um template jÃ¡ conhecido e realiza a extraÃ§Ã£o de forma autÃ´noma. Assim, os LLMs sÃ³ sÃ£o acionados em casos totalmente novos ou ambÃ­guos, enquanto o pipeline se torna cada vez mais rÃ¡pido, preciso e barato, reduzindo custos, latÃªncia e dependÃªncia de inferÃªncia externa a cada nova execuÃ§Ã£o. Cada novo documento fortalece esse processo.

Para uma breve introduÃ§Ã£o (em vÃ­deo!) sobre a minha soluÃ§Ã£o, clique [aqui](https://www.loom.com/share/3532c5e7c6154302845fb1ff431550c1)

![Rascunho do projeto](images/rascunho_projeto.jpeg)

### InstalaÃ§Ã£o

PrÃ©â€‘requisitos:
- macOS/Linux com bash/zsh (comandos abaixo)

Passo a passo:

```bash
# 1) Instale o uv (se ainda nÃ£o tiver)
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2) No diretÃ³rio do projeto, crie e ative o ambiente 3.13
uv venv -p 3.13
source .venv/bin/activate  # zsh/bash

# 3) Instale dependÃªncias com uv
uv pip install -r requirements.txt

# 4) Configure sua API key da OpenAI
export OPENAI_API_KEY="sua_chave"
# (opcional) ou use um .env:
echo "OPENAI_API_KEY=sua_chave" > .env
```

#### TL;DR

```bash
uv venv -p 3.13 && source .venv/bin/activate
uv pip install -r requirements.txt
export OPENAI_API_KEY="sua_chave"
python main.py dataset.json
```

### Como usar

```bash
# ExecuÃ§Ã£o bÃ¡sica (serial)
python main.py dataset.json

# SaÃ­da verbosa
python main.py dataset.json -v

# Paralelizar por label
python main.py dataset.json --parallel

# Medir tempos/uso de LLM e imprimir resumo
python main.py dataset.json --benchmark

# Desativar cache de resultados
python main.py dataset.json --no-cache

# Note que essas opÃ§Ãµes podem ser combinadas, como em
python main.py dataset.json --no-cache --parallel --benchmark
```

SaÃ­das:
- Arquivo por PDF em `results/<nome_arquivo>.json` (apenas os campos do schema)
- ConsolidaÃ§Ã£o incremental em `results/output.json`

### Formato de entrada (dataset.json)

```json
[
  {
    "label": "carteira_oab",
    "extraction_schema": {
      "nome": "Nome do profissional",
      "inscricao": "NÃºmero de inscriÃ§Ã£o",
      "situacao": "SituaÃ§Ã£o do profissional"
    },
    "pdf_path": "files/oab_1.pdf"
  }
]
```

#### Formato de saÃ­da (para cada PDF)

```json
{
  "nome": "MARIA SILVA",
  "inscricao": "123456",
  "situacao": "Ativo"
}
```

### Resultados
![Benchmarks](images/benchmarks.jpeg)
Benchmarks alcanÃ§ados nos datasets de CNH e RG - dois documentos com o que chamamos de estrutura rÃ­gida.

Resultados para um dataset que combina documentos do tipo CNH e RG:
![Benchmarks com asterisco](images/benchmarks_asterisco.jpeg)
Nota: para fins demonstrativos, a prova de que, com paralelismo, o tempo de extraÃ§Ã£o Ã© limitado inferiormente pelo tempo de extraÃ§Ã£o da fila de documentos mais demorada.
Nota 2: sim, o tempo do benchmark estÃ¡ quebrado por causa do paralelismo ðŸ˜… No entanto, Ã© possÃ­vel ver no terminal o tempo real que o programa levou para sua execuÃ§Ã£o.

![AcurÃ¡cia](images/acuracia.jpeg)

### CNH? RG? Coisas extras que usei e que nÃ£o cabem neste repositÃ³rio
Com base no [Brazilian-Identity-Document-Dataset](https://github.com/ricardobnjunior/Brazilian-Identity-Document-Dataset), processei e filtrei documentos (ilustrativos) que tinham um nÃ­vel razoavelmente bom de OCR. VocÃª pode ter acesso a esses (e outro!) dataset que criei para testar minha soluÃ§Ã£o para esse desafio [clicando aqui](https://drive.google.com/drive/folders/1ucgCUpDiWug4m9AWDIuOvDqoli5UNABh?usp=sharing).

Quer saber mais sobre esses extras? Assista e [esse outro vÃ­deo curto](https://www.loom.com/share/5ec592b7e6b1472984ead43d46e6cf37)

### Estrutura do projeto

```
enter_fellowship/
â”œâ”€â”€ box_parser.py                 # Segmenta caixas multi-campos e casa segmentos com campos
â”œâ”€â”€ cache.py                      # Cache de resultados + versionamento de cÃ³digo
â”œâ”€â”€ classification.py             # ClassificaÃ§Ã£o rÃ­gida/flexÃ­vel via LLM (com cache por label)
â”œâ”€â”€ evaluator.py                  # AvaliaÃ§Ã£o de acurÃ¡cia (results vs oracle_results)
â”œâ”€â”€ extraction.py                 # ExtraÃ§Ã£o de texto com/sem coordenadas (PyMuPDF)
â”œâ”€â”€ format_validator.py           # InferÃªncia/validaÃ§Ã£o de formatos e extraÃ§Ã£o de substrings vÃ¡lidas
â”œâ”€â”€ hashing.py                    # Hash perceptual estÃ¡vel (full/left/right) e matching de template
â”œâ”€â”€ heuristics.py                 # Matching posicional com pontuaÃ§Ã£o composta e truncamentos defensivos
â”œâ”€â”€ llm_utils.py                  # Cliente OpenAI e utilitÃ¡rios de classificaÃ§Ã£o/extraÃ§Ã£o via LLM
â”œâ”€â”€ main.py                       # CLI (parÃ¢metros, logging, delega ao pipeline)
â”œâ”€â”€ oracle.py                     # ExtraÃ§Ã£o via LLM + criaÃ§Ã£o/uso de templates em memÃ³ria
â”œâ”€â”€ pipeline.py                   # OrquestraÃ§Ã£o: cache â†’ classificaÃ§Ã£o â†’ extraÃ§Ã£o â†’ salvamento/benchmark
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ templates.json           # Conhecimento persistente por label (gerado/atualizado em runtime)
â”œâ”€â”€ results/                     # SaÃ­das por PDF e output.json (gerados em runtime)
â”‚   â””â”€â”€ *.json
â”œâ”€â”€ oracle_results/              # Gabaritos para avaliaÃ§Ã£o (opcional)
â”‚   â””â”€â”€ *.json
â”œâ”€â”€ files/                       # Exemplos pequenos de PDFs para teste rÃ¡pido
â”‚   â””â”€â”€ *.pdf
â”œâ”€â”€ dataset*.json               # Datasets de entrada (ex.: dataset.json, dataset_RG.json, ...)
â”œâ”€â”€ dataset_CNH/                 # PDFs grandes (ignorados no Git)
â”œâ”€â”€ dataset_RG/                  # PDFs grandes (ignorados no Git)
â””â”€â”€ dataset_compras_BNDES/       # PDFs grandes (ignorados no Git)
```



### Custo x acurÃ¡cia (resumo da estratÃ©gia)
- ClassificaÃ§Ã£o por label com cache (minimiza chamadas ao LLM).
- Hashing perceptual multiâ€‘regiÃ£o + templates em memÃ³ria para reuso imediato.
- HeurÃ­sticas posicionais e parsing de caixas multiâ€‘campos antes do LLM.
- LLM apenas para campos faltantes/invalidÃ¡veis (incremental) e para documentos flexÃ­veis.
- Conhecimento persistente por `label+campos` em `templates/templates.json` (tipos, comprimentos, padrÃµes, delimitadores).
- Cache de resultados em `results/`.

### Detalhes importantes
- O sistema tenta sempre heurÃ­sticas posicionais antes de recorrer ao LLM e chama o LLM apenas para campos faltantes/invalidÃ¡veis.
- Templates por hash vivem na memÃ³ria durante a execuÃ§Ã£o; o conhecimento agregado por `label` persiste em `templates/templates.json`.
- O cache invalida automaticamente quando PDFs mudam ou quando a versÃ£o do cÃ³digo de extraÃ§Ã£o se altera.

### Troubleshooting
- Erro de API: verifique `OPENAI_API_KEY` e conectividade; use `-v` para logs. Sem `-v` nada nunca serÃ¡ impresso (otimizaÃ§Ã£o para os benchmarks).
- Erros com PyMuPDF: cheque instalaÃ§Ã£o do `pymupdf` e permissÃµes de leitura do PDF.
- Sem resultados em `results/`: confirme caminhos do `dataset.json` e existÃªncia dos arquivos.

### AvaliaÃ§Ã£o de acurÃ¡cia (opcional)

```bash
python evaluator.py
```
Compara `results/*.json` com os gabaritos em `oracle_results/*.json` e imprime um resumo de acurÃ¡cia geral/por arquivo/campo. Para gerar o output do orÃ¡culo, tambÃ©m usei o Chat GPT 5 mini.
